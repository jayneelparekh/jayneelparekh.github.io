---
---

@article{parekh2023tackling,
  title={Tackling Interpretability in Audio Classification Networks with Non-negative Matrix Factorization},
  author={Parekh, Jayneel and Parekh, Sanjeel and Mozharovskyi, Pavlo and Richard, Ga{\"e}l and d'Alch{\'e}-Buc, Florence},
  journal={Accepted in IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP) (available via early access, preprint on arxiv)},
  year={2023},
  abstract={This paper tackles two major problem settings for interpretability of audio processing networks, post-hoc and by-design interpretation. For post-hoc interpretation, we aim to interpret decisions of a network in terms of high-level audio objects that are also listenable for the end-user. This is extended to present an inherently interpretable model with high performance. To this end, we propose a novel interpreter design that incorporates non-negative matrix factorization (NMF). In particular, an interpreter is trained to generate a regularized intermediate embedding from hidden layers of a target network, learnt as time-activations of a pre-learnt NMF dictionary. Our methodology allows us to generate intuitive audio-based interpretations that explicitly enhance parts of the input signal most relevant for a network's decision. We demonstrate our method's applicability on a variety of classification tasks, including multi-label data for real-world audio and music.},
  pdf={https://arxiv.org/pdf/2305.07132.pdf}
}

@article{parekh2022listen,
  title={Listen to Interpret: Post-hoc Interpretability for Audio Networks with NMF},
  author={Parekh, Jayneel and Parekh, Sanjeel and Mozharovskyi, Pavlo and d'Alch{\'e}-Buc, Florence and Richard, Ga{\"e}l},
  journal={Advances in Neural Information Processing Systems <b>(NeurIPS)</b>},
  year={2022},
  abstract={This paper tackles post-hoc interpretability for audio processing networks. Our goal is to interpret decisions of a trained network in terms of high-level audio objects that are also listenable for the end-user. To this end, we propose a novel interpreter design that incorporates non-negative matrix factorization (NMF). In particular, a regularized interpreter module is trained to take hidden layer representations of the targeted network as input and produce time activations of pre-learnt NMF components as intermediate outputs. Our methodology allows us to generate intuitive audio-based interpretations that explicitly enhance parts of the input signal most relevant for a network's decision. We demonstrate our method's applicability on popular benchmarks, including a real-world multi-label classification task.},
  pdf={https://arxiv.org/pdf/2202.11479.pdf}
}


@article{parekh2021framework,
  title={A Framework to Learn with Interpretation},
  author={Parekh, Jayneel and Mozharovskyi, Pavlo and d'Alch{\'e}-Buc, Florence},
  journal={Advances in Neural Information Processing Systems <b>(NeurIPS)</b>},
  volume={34},
  pages={24273--24285},
  year={2021},
  abstract={To tackle interpretability in deep learning, we present a novel framework to jointly learn a predictive model and its associated interpretation model. The interpreter provides both local and global interpretability about the predictive model in terms of human-understandable high level attribute functions, with minimal loss of accuracy. This is achieved by a dedicated architecture and well chosen regularization penalties. We seek for a small-size dictionary of high level attribute functions that take as inputs the outputs of selected hidden layers and whose outputs feed a linear classifier. We impose strong conciseness on the activation of attributes with an entropy-based criterion while enforcing fidelity to both inputs and outputs of the predictive model. A detailed pipeline to visualize the learnt features is also developed. Moreover, besides generating interpretable models by design, our approach can be specialized to provide post-hoc interpretations for a pre-trained neural network. We validate our approach against several state-of-the-art methods on multiple datasets and show its efficacy on both kinds of tasks},
  pdf={https://proceedings.neurips.cc/paper/2021/file/cbb6a3b884f4f88b3a8e3d44c636cbd8-Paper.pdf} 
}

@article{beaudouin2020flexible,
  title={Flexible and context-specific AI explainability: A multidisciplinary approach},
  author={Beaudouin, Val{\'e}rie and Bloch, Isabelle and Bounie, David and Cl{\'e}men{\c{c}}on, St{\'e}phan and d'Alch{\'e}-Buc, Florence and Eagan, James and Maxwell, Winston and Mozharovskyi, Pavlo and Parekh, Jayneel},
  journal={arXiv Preprint, shorter version presented at NeHuAI@ECAI},
  year={2020},
  abstract={The recent enthusiasm for artificial intelligence (AI) is due principally to advances in deep learning. Deep learning methods are remarkably accurate, but also opaque, which limits their potential use in safety-critical applications. To achieve trust and accountability, designers and operators of machine learning algorithms must be able to explain the inner workings, the results and the causes of failures of algorithms to users, regulators, and citizens. The originality of this paper is to combine technical, legal and economic aspects of explainability to develop a framework for defining the ”right” level of explainability in a given context. We propose three logical steps: First, define the main contextual factors, such as who the audience of the explanation is, the operational context, the level of harm that the system could cause, and the legal/regulatory framework. This step will help characterize the operational and legal needs for explanation, and the corresponding social benefits. Second, examine the technical tools available, including post hoc approaches (input perturbation, saliency maps...) and hybrid AI approaches. Third, as function of the first two steps, choose the right levels of global and local explanation outputs, taking into the account the costs involved. We identify seven kinds of costs and emphasize that explanations are socially useful only when total social benefits exceed costs.},
  pdf={https://arxiv.org/pdf/2003.07703.pdf}
}


@article{parekh2020speech,
  title={Speech-to-Singing Conversion in an Encoder-Decoder Framework},
  author={Parekh, Jayneel and Rao, Preeti and Yang, Yi-Hsuan},
  journal={IEEE International Conference on Acoustics, Speech and Signal Processing <b>(ICASSP)</b>},
  pages={261--265},
  year={2020},
  abstract={In this paper our goal is to convert a set of spoken lines into sung ones. Unlike previous signal processing based methods, we take a learning based approach to the problem. This allows us to automatically model various aspects of this transformation, thus overcoming dependence on specific inputs such as high quality singing templates or phoneme-score synchronization information. Specifically, we propose an encoder–decoder framework for our task. Given timefrequency representations of speech and a target melody contour, we learn encodings that enable us to synthesize singing that preserves the linguistic content and timbre of the speaker while adhering to the target melody. We also propose a multi-task learning based objective to improve lyric intelligibility. We present a quantitative and squalitative analysis of our framework.},
  pdf={https://arxiv.org/pdf/2002.06595.pdf},
  website={https://jayneelparekh.github.io/icassp20/},
  prestype={(<b>Oral</b>)},
  event={Conference}
}


@article{parekh2018deep,
  title={Deep Pairwise Classification and Ranking for Predicting Media Interestingness},
  author={Parekh, Jayneel and Tibrewal, Harshvardhan and Parekh, Sanjeel},
  journal={ACM International Conference on Multimedia Retrieval <b>(ICMR)</b>},
  pages={428--433},
  year={2018},
  abstract={With the explosive increase in the consumption of multimediacontent in recent years, the eld of media interestingness analysishas gained a lot of attention. This paper tackles the problem of imageinterestingness in videos and proposes a novel algorithm basedon pairwise-comparisons of frames to rank all frames in a video.Experiments performed on the Predicting Media Interestingnessdataset, arm its eectiveness over existing solutions. In terms ofthe ocial metric i.e. Mean Average Precision at 10, it outperformsthe previous state-of-the-art (to the best of our knowledge) on thisdataset. Additional results on video interestingness substantiatethe exibility and performance reliability of our approach.},
  pdf={https://www.researchgate.net/publication/325705259_Deep_Pairwise_Classification_and_Ranking_for_Predicting_Media_Interestingness}
}

@article{parekh2017iitb,
  title={The IITB Predicting Media Interestingness System for MediaEval 2017.},
  author={Parekh, Jayneel and Tibrewal, Harshvardhan and Parekh, Sanjeel},
  journal={MediaEval Workshop},
  year={2017}
}


@article{parekh2016mlpboon,
  title={The MLPBOON Predicting Media Interestingness System for MediaEval 2016.},
  author={Parekh, Jayneel and Parekh, Sanjeel},
  journal={MediaEval Workshop},
  year={2016}
}


